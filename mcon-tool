#!/usr/bin/env python3

import sys
import argparse
import json
import csv
import logging
import copy
from collections import defaultdict

parser = argparse.ArgumentParser(description="Convert MCON files")
parser.add_argument("filename",nargs='?', default=None,help="file to convert")
parser.add_argument("--unnest",help="convert nested MCON -> non-nested MCON", action='store_true')
parser.add_argument("--atomize",help="split nested values", action='store_true')
parser.add_argument("--drop",nargs='+',help="split nested values")
#parser.add_argument("--nest",help="convert non-nested MCON -> nested MCON", action='store_true')
parser.add_argument("-O","--output",default="MCON",help="output format")
parser.add_argument("-V","--verbose",help="output format", action='store_true')
args = parser.parse_args()

def sort_key(name):
    # Should we forbid the data keys from containing '[' and ']'?
    keylist = name.split('[')
    for i in range(len(keylist)):
        if i != 0 and keylist[i].endswith(']'):
            keylist[i] = keylist[i][:-1]
            # Returning (0,int) and (1,str) avoids doing int < str.
            try:
                ikey = int(keylist[i])
                keylist[i] = (0,ikey)
            except ValueError:
                keylist[i] = (1,keylist[i])
    return keylist

def is_nested_key(key):
    assert(isinstance(key,str))
    if len(key) > 0 and key.endswith('/'):
        return True
    else:
        return False

def get_keys_non_nested(sample):
    keys = set()
    for key in sample:
        keys.add(key)
    return keys

def get_keys_nested(sample):
    keys = set()
    for key in sample:
        if is_nested_key(key):
            keys2 = get_keys_nested(sample[key])
            for key2 in keys2:
                keys.add(key + key2)
        else:
            keys.add(key)
    return keys


def get_long_names_and_values_data(prefix, value):
    d = dict()
    if isinstance(value,dict):
        for k,v in value.items():
            # What if the keys contain ']'?
            k2 = k.replace('[','<').replace(']','>')
            d.update(get_long_names_and_values_data(prefix+'['+k2+']', v))
    elif isinstance(value,list):
        for i in range(len(value)):
            d.update(get_long_names_and_values_data(prefix+'['+str(i+1)+']', value[i]))
    else:
        d[prefix] = value
    return d

def atomize(sample, nested):
    sample2 = dict()

    for k,v in sample.items():
        if nested and is_nested_key(k):
            sample3 = dict()
            for k2,v2 in atomize(v,True).items():
                sample3[k2] = v2
            sample2[k] = sample3
        else:
            sample2.update(get_long_names_and_values_data(k,v))

    return sample2

def unnest(sample, prefix=""):
    sample2 = dict()
    for k,v in sample.items():
        if is_nested_key(k):
            sample2.update(unnest(v,prefix+k))
        else:
            sample2[prefix+k] = v
    return sample2

def nest(d):
    d2 = dict()

    subgroups = defaultdict(dict)
    for k,v in d.items():
        if "/" in k:
            prefix,rest = k.split("/",1)
            prefix += "/"
            subgroups[prefix][rest] = v
        else:
            d2[k] = v

    for k,v in subgroups.items():
        d2[k] = nest(v)

    return d2

def chop_nested(k):
    if is_nested_key(k):
        return k[0:len(k)-1]
    else:
        return k

def simplify(m):
    m2 = dict()

    # Mark keys as seen
    for k,v in m.items():
        if is_nested_key(k):
            m2[k] = simplify(v)
        else:
            m2[k] = v

    seen = defaultdict(int)

    # Mark keys as seen
    for k,v in m2.items():
        seen[chop_nested(k)] += 1
        if is_nested_key(k):
            for k2 in v:
                seen[chop_nested(k2)] += 1

    m3 = dict()
    for k,v in m2.items():
        if is_nested_key(k):
            ok = True
            for k2 in v:
                if seen[k2] > 1:
                    ok = False
            if ok:
                m3.update(v)
            else:
                m3[k] = v
        else:
            m3[k] = v

    return m3

def shortFields(fields):
    # 1. Construct the mapping from short field names to position.
    m = dict()
    for i in range(len(fields)):
        m[fields[i]] = i

    m = unnest(simplify(nest(m)))

    # 2. Construct the ORDERED list of short field names.
    fields2 = ["" for i in range(len(fields))]
    for name,index in m.items():
        fields2[index] = name

    return fields2

def findConstantFields(j1, j2):
#    print(f"   findConstantFieldsIn: {j1}   <=>   {j2}")
    if j1 is None:
        return None
    elif isinstance(j1, dict):
        if not isinstance(j2, dict):
            return None
        if j1.keys() != j2.keys():
            return None
        for key in j1.keys():
            j1[key] = findConstantFields(j1[key], j2[key])
#        print(f"   findConstantFieldsOut: {j1}   <=>   {j2}")
        return j1
    elif isinstance(j1, list):
        if not isinstance(j2, list):
            return None
        if len(j1) != len(j2):
            return None
        for i in range(0,len(j1)):
            j1[i] = findConstantFields(j1[i], j2[i])
#        print(f"   findConstantFieldsOut: {j1}   <=>   {j2}")
        return j1
    elif isinstance(j1, str):
        if not isinstance (j2, str):
            return None
    elif isinstance(j1,bool):
        if not isinstance (j2, bool):
            return None
    elif isinstance (j1, (int,float)):
        if not isinstance (j2, (int,float)):
            return None
    else:
        print(f"I don't recognize object '{j1}'")
        j1 = None
    return j1

def classifyFields(samples):
    fields = copy.deepcopy(samples[0])
    for i in range(1,len(samples)):
        fields = findConstantFields(fields, samples[i])

    fields2 = atomize(unnest(fields),False)
    constantFields = set()
    varyingFields = set()
    for key in fields2.keys():
        if fields2[key] is None:
            varyingFields.add(key)
        else:
            constantFields.add(key)

    return (constantFields, varyingFields)

            
class LogFile(object):
    def __init__(self, header, samples):
        self.fields = header.get("fields",None)
        self.nested = header.get("nested",True)
        self.atomic = header.get("atomic",False)
        self.samples = samples
        self.constantFields = None
        
    def is_nested(self):
        return self.nested

    def is_atomic(self):
        return self.atomic

    def dump_MCON(self,**kwargs):
        header = dict()
        if self.fields is not None:
            header["fields"] = self.fields
        header["format"] = "MCON"
        header["version"] = "0.1"
        header["nested"] = self.nested
        header["atomic"] = self.atomic
        print(json.dumps(header),**kwargs)
        for sample in self.samples:
            print(json.dumps(sample),**kwargs)

    def get_keys(self):
        if self.nested:
            return get_keys_nested(self.samples[0])
        else:
            return get_keys_non_nested(self.samples[0])

    def unnest(self):
        if self.is_nested():
            logging.debug("Unnesting samples")
            samples2 = []
            for sample in self.samples:
                samples2.append(unnest(sample))
            self.samples = samples2
            self.nested = False

    def atomize(self):
        if not self.atomic:
            logging.debug("Atomizing samples")
            samples2 = []
            for sample in self.samples:
                samples2.append(atomize(sample,self.nested))
                self.samples = samples2
            self.atomic = True

    def drop(self, name):
        for sample in self.samples:
            sample.pop(name,None)


    def dump_TSV(self,**kwargs):
        # We are actually writing TSJ -- no quoting.
        # writer = csv.writer(kwargs["file"], delimiter='\t', quoting=csv.QUOTE_NONE)

        if self.is_nested() or not self.is_atomic():
            (constantFields, varyingFields) = classifyFields(self.samples)
            if varyingFields:
                print(f"varying fields = {' '.join(varyingFields)}", file=sys.stderr)
            logfile = copy.deepcopy(self)
            logfile.unnest()
            logfile.atomize()
            logfile.constantFields = constantFields
            logfile.dump_TSV(**kwargs)
            return

        # We should be fully flattened if we get here.
        if "shortNames" not in kwargs:
            kwargs["shortNames"] = True

        fields = []
        if self.fields is not None:
            fields = self.fields
        fields1=set(fields)
        all_fields = self.get_keys()
        if self.constantFields is None:
            self.constantFields = all_fields

        for field in fields1:
            if field not in all_fields:
                print(f"Error: Header: field '{field}' does not exist!",file=sys.stderr)
                print(f"  Existing fields include: {all_fields}")
                exit(1)
            if field not in self.constantFields:
                print(f"Error: Header: field '{field}' does not have a fixed structure!",file=sys.stderr)
                print(f"  Fixed structure fields include: {self.constantFields}")
                exit(1)


        fields2 = []
        for field in self.constantFields:
            if field not in fields1:
                fields2.append(field)
        fields2.sort(key=sort_key)
        fields += fields2

        nfields = len(fields)
        logging.debug(f"Writing TSV: {nfields} fields")

        printed_fields = fields
        if "shortNames" in kwargs and kwargs["shortNames"]:
            printed_fields = shortFields(fields)

        print('\t'.join(printed_fields))

        sample_number = 0
        for sample in self.samples:
            sample_number += 1
            row = []
            for i in range(nfields):
                if fields[i] not in sample:
                    print(f"Error: sample line {sample_number} is missing field '{fields[i]}'", file=sys.stderr)
                    print(f"  {sample}", file=sys.stderr)
                    exit(1)
                row.append(json.dumps(sample[fields[i]]))
#            if len(sample) != nfields:
#                for k,v in sample.items():
#                    if k not in all_fields:
#                        print(f"Error: sample line {sample_number} has extra field '{k}'", file=sys.stderr);
#                        print(f"  {sample}", file=sys.stderr)
#                        exit(1)
            print('\t'.join(row))

# Formats should be MCON/JSON, TSV, TSJ, CSV, CSJ
def detect_format(infile):
    firstline = infile.readline().rstrip()
    # this shouldn't have any chars that need to be escaped in TSV.
    # assert( ??? )

    
    # want unquoted TSV.  Therefore, we can just read the first line 
    try:
        header = json.loads(firstline)
        logging.debug("JSON detected")
        if "format" in header and header["format"] == "MCON" and "version" in header:
            logging.debug("MCON detected")
            return ("MCON", firstline)
        else:
            # If this is JSON, then its not TSV
            logging.debug("Not an MCON file.")
            return None
    except Exception as e:
        logging.debug("TSV detected")
        logging.debug(f"exception {e}")
        return ("TSV", firstline)
    
def read_MCON(firstline,infile):
    logging.debug("reading MCON")
    header = json.loads(firstline)
    samples = []
    for line in infile:
        line = line.strip()
        if line:
            samples.append(json.loads(line))
    return LogFile(header, samples)

def read_TSV(firstline, infile):
    logging.debug("reading TSV")
    tsv_fields = firstline.split('\t')
    nfields = len(tsv_fields)

    header = dict()
    header["format"] = "MCON"
    header["version"] = "0.1"
    header["fields"] = tsv_fields
    header["nested"] = False

    reader = csv.reader(infile, delimiter="\t", quotechar='"')

    samples = []
    ignored_lines = 0;
    for tsv_line in reader:
        if len(tsv_line) != nfields:
            print(f"TSV line {1+len(samples)} has {len(tsv_line)} fields, but header has {nfields} fields.", file=sys.stderr)
            ignored_lines += 1
            break
        j = dict()
        for i in range(nfields):
            j[tsv_fields[i]] = json.loads(tsv_line[i])
        samples.append(j)

    for tsv_line in reader:
        ignored_lines += 1

    if ignored_lines > 0:
        print(f"Skipping the next {ignored_lines} lines.", file=sys.stderr)

    print(f"Read {len(samples)} samples of TSV with {nfields} fields.", file=sys.stderr)
    return LogFile(header, samples)

def read_logfile(infile,filename):
    format, firstline  = detect_format(infile)
    if format == "MCON":
        return read_MCON(firstline, infile)
    elif format == "TSV":
        return read_TSV(firstline, infile)
    elif format is None:
        print(f"Unknown format for file '{filename}'")
    else:
        print(f"Unknown format {format} for file '{filename}'")
    exit(1)

if args.verbose:
    logging.basicConfig(level=logging.DEBUG)

infile = sys.stdin
if args.filename is not None:
    infile = open(args.filename)

logfile = read_logfile(infile, args.filename)

if args.drop is not None:
    for name in args.drop:
        logfile.drop(name)

if args.unnest:
    logfile.unnest()

if args.atomize:
    logfile.atomize()

if args.output == "tsv" or args.output == "TSV":
    logfile.dump_TSV(file=sys.stdout)
elif args.output == "mcon" or args.output == "MCON":
    logfile.dump_MCON(file=sys.stdout)
else:
    print(f"Error: Unrecognized output format '{args.output}'")
